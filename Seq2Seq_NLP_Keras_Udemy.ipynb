{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq NLP Keras Udemy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYgpfIQxQOtoSCQZDzYI0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonVillanueva/CoLab/blob/master/Seq2Seq_NLP_Keras_Udemy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjvsUwjDlD_x"
      },
      "source": [
        "# # https://deeplearningcourses.com/c/deep-learning-advanced-nlp\r\n",
        "# get the data at: http://www.manythings.org/anki/\r\n",
        "from __future__ import print_function, division\r\n",
        "from builtins import range, input\r\n",
        "# Note: you may need to update your version of future\r\n",
        "# sudo pip install -U future\r\n",
        "\r\n",
        "import os, sys\r\n",
        "\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "try:\r\n",
        "  import keras.backend as K\r\n",
        "  if len(K.tensorflow_backend._get_available_gpus()) > 0:\r\n",
        "    from keras.layers import CuDNNLSTM as LSTM\r\n",
        "    from keras.layers import CuDNNGRU as GRU\r\n",
        "except:\r\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckbt1SYslWd6"
      },
      "source": [
        "BATCH_SIZE = 64  # Batch size for training.\r\n",
        "EPOCHS = 40  # Number of epochs to train for.\r\n",
        "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\r\n",
        "NUM_SAMPLES = 10000  # Number of samples to train on.\r\n",
        "MAX_NUM_WORDS = 20000\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "\r\n",
        "# Where we will store the data\r\n",
        "input_texts = [] # sentence in original language\r\n",
        "target_texts = [] # sentence in target language\r\n",
        "target_texts_inputs = [] # sentence in target language offset by 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yHizSRzlbJa",
        "outputId": "9d989b04-ca02-4030-933d-412e4851725c"
      },
      "source": [
        "t = 0\r\n",
        "for line in open('sample_data/spa.txt'):\r\n",
        "  # only keep a limited number of samples\r\n",
        "  t += 1\r\n",
        "  if t > NUM_SAMPLES:\r\n",
        "    break\r\n",
        "\r\n",
        "  # input and target are separated by tab\r\n",
        "  if '\\t' not in line:\r\n",
        "    continue\r\n",
        "\r\n",
        "  # split up the input and translation\r\n",
        "  input_text, translation, *rest = line.rstrip().split('\\t')\r\n",
        "\r\n",
        "  # make the target input and output\r\n",
        "  # recall we'll be using teacher forcing\r\n",
        "  target_text = translation + ' <eos>'\r\n",
        "  target_text_input = '<sos> ' + translation\r\n",
        "\r\n",
        "  input_texts.append(input_text)\r\n",
        "  target_texts.append(target_text)\r\n",
        "  target_texts_inputs.append(target_text_input)\r\n",
        "print(\"num samples:\", len(input_texts))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeHT5xIcnjAS",
        "outputId": "9d33772d-f498-4652-a15a-ac985c0e90bf"
      },
      "source": [
        "# tokenize the inputs\r\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\r\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\r\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\r\n",
        "\r\n",
        "# get the word to index mapping for input language\r\n",
        "word2idx_inputs = tokenizer_inputs.word_index\r\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2355 unique input tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Yia-_hno_u"
      },
      "source": [
        "# determine maximum length input sequence\r\n",
        "max_len_input = max(len(s) for s in input_sequences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZl_4g-YntE8"
      },
      "source": [
        "# tokenize the outputs\r\n",
        "# don't filter out special characters\r\n",
        "# otherwise <sos> and <eos> won't appear\r\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\r\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\r\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\r\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5kLHlJooEvt",
        "outputId": "e9dd58e4-1ec5-4105-e361-1813bed9d689"
      },
      "source": [
        "# get the word to index mapping for output language\r\n",
        "word2idx_outputs = tokenizer_outputs.word_index\r\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\r\n",
        "\r\n",
        "# store number of output words for later\r\n",
        "# remember to add 1 since indexing starts at 1\r\n",
        "num_words_output = len(word2idx_outputs) + 1\r\n",
        "\r\n",
        "# determine maximum length output sequence\r\n",
        "max_len_target = max(len(s) for s in target_sequences)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6335 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GVM4y9coF41",
        "outputId": "1925974b-c74e-4219-ae95-12aece195959"
      },
      "source": [
        "# pad the sequences\r\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\r\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\r\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\r\n",
        "\r\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\r\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\r\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\r\n",
        "\r\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_inputs.shape: (10000, 5)\n",
            "encoder_inputs[0]: [ 0  0  0  0 12]\n",
            "decoder_inputs[0]: [   2 1463    0    0    0    0    0    0    0]\n",
            "decoder_inputs.shape: (10000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "RKW5L334oJu9",
        "outputId": "8a078b7e-5495-4a94-e0cf-8079f19cc05d"
      },
      "source": [
        "# store all the pre-trained word vectors\r\n",
        "print('Loading word vectors...')\r\n",
        "word2vec = {}\r\n",
        "with open(os.path.join('sample_data/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\r\n",
        "  # is just a space-separated text file in the format:\r\n",
        "  # word vec[0] vec[1] vec[2] ...\r\n",
        "  for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    vec = np.asarray(values[1:], dtype='float32')\r\n",
        "    word2vec[word] = vec\r\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e9be055a2834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s word vectors.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '-'"
          ]
        }
      ]
    }
  ]
}