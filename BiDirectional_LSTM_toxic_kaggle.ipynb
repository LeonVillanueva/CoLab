{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiDirectional_LSTM_toxic_kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Gxx1PqWQ_5rXch0H0Y7cP97-AqCtDgTS",
      "authorship_tag": "ABX9TyN87EZLr1A4sLTW42YJBxhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonVillanueva/CoLab/blob/master/BiDirectional_LSTM_toxic_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtcwOUVtiChO",
        "outputId": "faf090c1-0bf9-45d8-d8a5-da95b5d4f030"
      },
      "source": [
        "# https://deeplearningcourses.com/c/deep-learning-advanced-nlp\r\n",
        "from __future__ import print_function, division\r\n",
        "from builtins import range\r\n",
        "# Note: you may need to update your version of future\r\n",
        "# sudo pip install -U future\r\n",
        "\r\n",
        "\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Dense, Embedding, Input\r\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.optimizers import Adam\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "import keras.backend as K\r\n",
        "# if len(K.tensorflow_backend._get_available_gpus()) > 0:\r\n",
        "#   from keras.layers import CuDNNLSTM as LSTM\r\n",
        "#   from keras.layers import CuDNNGRU as GRU\r\n",
        "\r\n",
        "\r\n",
        "# Download the data:\r\n",
        "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\r\n",
        "# Download the word vectors:\r\n",
        "# http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "\r\n",
        "\r\n",
        "# some configuration\r\n",
        "MAX_SEQUENCE_LENGTH = 100\r\n",
        "MAX_VOCAB_SIZE = 20000\r\n",
        "EMBEDDING_DIM = 50\r\n",
        "VALIDATION_SPLIT = 0.2\r\n",
        "BATCH_SIZE = 128\r\n",
        "EPOCHS = 5\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# load in pre-trained word vectors\r\n",
        "print('Loading word vectors...')\r\n",
        "word2vec = {}\r\n",
        "with open(os.path.join('../content/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\r\n",
        "  # is just a space-separated text file in the format:\r\n",
        "  # word vec[0] vec[1] vec[2] ...\r\n",
        "  for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    vec = np.asarray(values[1:], dtype='float32')\r\n",
        "    word2vec[word] = vec\r\n",
        "print('Found %s word vectors.' % len(word2vec))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# prepare text samples and their labels\r\n",
        "print('Loading in comments...')\r\n",
        "\r\n",
        "train = pd.read_csv(\"../content/Toxic_NLP_train.csv\")\r\n",
        "sentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\r\n",
        "possible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\r\n",
        "targets = train[possible_labels].values\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# convert the sentences (strings) into integers\r\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# get word -> integer mapping\r\n",
        "word2idx = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word2idx))\r\n",
        "\r\n",
        "\r\n",
        "# pad sequences so that we get a N x T matrix\r\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "print('Shape of data tensor:', data.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# prepare embedding matrix\r\n",
        "print('Filling pre-trained embeddings...')\r\n",
        "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\r\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\r\n",
        "for word, i in word2idx.items():\r\n",
        "  if i < MAX_VOCAB_SIZE:\r\n",
        "    embedding_vector = word2vec.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "      # words not found in embedding index will be all zeros.\r\n",
        "      embedding_matrix[i] = embedding_vector\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# load pre-trained word embeddings into an Embedding layer\r\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\r\n",
        "embedding_layer = Embedding(\r\n",
        "  num_words,\r\n",
        "  EMBEDDING_DIM,\r\n",
        "  weights=[embedding_matrix],\r\n",
        "  input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "  trainable=False\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print('Building model...')\r\n",
        "\r\n",
        "# create an LSTM network with a single LSTM\r\n",
        "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\r\n",
        "x = embedding_layer(input_)\r\n",
        "# x = LSTM(15, return_sequences=True)(x)\r\n",
        "x = Bidirectional(LSTM(15, return_sequences=True))(x)\r\n",
        "x = GlobalMaxPool1D()(x)\r\n",
        "output = Dense(len(possible_labels), activation=\"sigmoid\")(x)\r\n",
        "\r\n",
        "model = Model(input_, output)\r\n",
        "model.compile(\r\n",
        "  loss='binary_crossentropy',\r\n",
        "  optimizer=Adam(lr=0.01),\r\n",
        "  metrics=['accuracy'],\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "print('Training model...')\r\n",
        "r = model.fit(\r\n",
        "  data,\r\n",
        "  targets,\r\n",
        "  batch_size=BATCH_SIZE,\r\n",
        "  epochs=EPOCHS,\r\n",
        "  validation_split=VALIDATION_SPLIT\r\n",
        ")\r\n",
        "\r\n",
        "# plot some data\r\n",
        "plt.plot(r.history['loss'], label='loss')\r\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# accuracies\r\n",
        "plt.plot(r.history['accuracy'], label='acc')\r\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "p = model.predict(data)\r\n",
        "aucs = []\r\n",
        "for j in range(6):\r\n",
        "    auc = roc_auc_score(targets[:,j], p[:,j])\r\n",
        "    aucs.append(auc)\r\n",
        "print(np.mean(aucs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n",
            "Loading in comments...\n",
            "Found 210337 unique tokens.\n",
            "Shape of data tensor: (159571, 100)\n",
            "Filling pre-trained embeddings...\n",
            "Building model...\n",
            "Training model...\n",
            "Epoch 1/5\n",
            "184/998 [====>.........................] - ETA: 1:01 - loss: 0.1481 - accuracy: 0.9146"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}